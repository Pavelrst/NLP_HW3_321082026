\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbm}
\usepackage{tikz}

\usetikzlibrary{shapes}
\usetikzlibrary{automata, positioning, arrows}

\usepackage[shortlabels]{enumitem}
\usepackage[noabbrev, capitalise]{cleveref}

\usepackage{geometry}
 \geometry{
 a4paper,
 top=20mm,
 bottom=25mm,
 left=25mm,
 right=25mm,
 }

% define your IDs here:
\newcommand{\firststudentid}{341438711}
\newcommand{\secondstudentid}{321082026}

\pagestyle{fancy}
\fancyhf{}
\rhead{Written Solution for Assignment 3}
\chead{\firststudentid \qquad \secondstudentid}
\lhead{Natural Language Processing}
\rfoot{Page \thepage \hspace{1pt} of \pageref{LastPage}}
\renewcommand{\footrulewidth}{1pt}
 
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\renewcommand{\baselinestretch}{1.25}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\roman{subsubsection}}

\begin{document}

\section*{1. Data Preprocessing}

(no written deliverable required)

\section*{2. Most Frequent Tag Baseline}

\begin{itemize}
    \item[(a)] (no written deliverable required)
    \item[(b)] 

$F_1$ score on dev set: 0.80

Full output reproduced here for completeness:

\begin{verbatim}
Token-level confusion matrix:
go\gu   	PER     	ORG     	LOC     	MISC    	O       
PER     	3033.00 	20.00   	29.00   	5.00    	62.00   
ORG     	494.00  	1139.00 	157.00  	43.00   	259.00  
LOC     	287.00  	146.00  	1510.00 	21.00   	130.00  
MISC    	219.00  	29.00   	52.00   	842.00  	126.00  
O       	290.00  	46.00   	6.00    	27.00   	42390.00

Token-level scores:
label	acc  	prec 	rec  	f1   
PER  	0.97 	0.70 	0.96 	0.81 
ORG  	0.98 	0.83 	0.54 	0.66 
LOC  	0.98 	0.86 	0.72 	0.78 
MISC 	0.99 	0.90 	0.66 	0.76 
O    	0.98 	0.99 	0.99 	0.99 
micro	0.98 	0.95 	0.95 	0.95 
macro	0.98 	0.85 	0.78 	0.80 
not-O	0.98 	0.78 	0.76 	0.77 

Entity level P/R/F1: 0.78/0.82/0.80
\end{verbatim}

\end{itemize}

\section*{3. HMM Tagger}

\begin{itemize}
    \item[(a)] (no written deliverable required)
    \item[(b)] Our pruning policy is to drop out a pair of tags with lowest value in each layer. Layers smaller than 4 are not pruned. The pruning function implementation in pseudo-code:
    \begin{verbatim}
        def prune(self):
        if len(self.layer_dict) > MIN_LAYER_SIZE_TO_PRUNE:
            weak_key = key with lowest value
            del self.layer_dict[weak_key]
        return
    \end{verbatim}
   
    \item[(c)] $F_1$ on dev set: 0.84
    
    Full output of hmm.py:

    \begin{verbatim}
Token-level confusion matrix:
go\gu   	PER     	ORG     	LOC     	MISC    	O       
PER     	3036.00 	56.00   	19.00   	12.00   	26.00   
ORG     	327.00  	1467.00 	125.00  	41.00   	132.00  
LOC     	271.00  	227.00  	1542.00 	25.00   	29.00   
MISC    	180.00  	58.00   	19.00   	930.00  	81.00   
O       	268.00  	234.00  	15.00   	76.00   	42166.00

Token-level scores:
label	acc  	prec 	rec  	f1   
PER  	0.98 	0.74 	0.96 	0.84 
ORG  	0.98 	0.72 	0.70 	0.71 
LOC  	0.99 	0.90 	0.74 	0.81 
MISC 	0.99 	0.86 	0.73 	0.79 
O    	0.98 	0.99 	0.99 	0.99 
micro	0.98 	0.96 	0.96 	0.96 
macro	0.98 	0.84 	0.82 	0.83 
not-O	0.98 	0.78 	0.81 	0.80 

Entity level P/R/F1: 0.81/0.86/0.84
    \end{verbatim}

    \item[(d)] \begin{verbatim} P(x_8 = 'LOC') = 0.024332529500131664 \end{verbatim}
    \item[(e)]
\end{itemize}

\section*{4. MEMM Tagger}

\begin{itemize}
    \item[(a)] (no written deliverable required)
    \item[(b)] (no written deliverable required)
    \item[(c)] Optimizations used:
    \begin{itemize}

    \item At each step, the predict(...) function of LogisticRegression is called on all candidates at once (one feature matrix where each row is a candidate path). This is more efficient than calling it on each candidate path separately.
    \item Scores and candidate paths are cached in the variable best\textunderscore paths
    \item For numerical stability, summed log-probabilities are used instead of the product of probabilities.
    \item Paths (partially tagged sentences) that are not extended on a step are pruned (overwritten).
    \end{itemize}

    \item[(d)] $F_1$ on dev set: 0.85 for both greedy and Viterbi decoding.
    
Greedy decoding output:

\begin{verbatim}
    Token-level confusion matrix:
go\gu   	PER     	ORG     	LOC     	MISC    	O       
PER     	2778.00 	53.00   	71.00   	22.00   	225.00  
ORG     	108.00  	1607.00 	80.00   	44.00   	253.00  
LOC     	41.00   	101.00  	1729.00 	27.00   	196.00  
MISC    	40.00   	38.00   	25.00   	1004.00 	161.00  
O       	55.00   	87.00   	42.00   	64.00   	42511.00

Token-level scores:
label	acc  	prec 	rec  	f1   
PER  	0.99 	0.92 	0.88 	0.90 
ORG  	0.99 	0.85 	0.77 	0.81 
LOC  	0.99 	0.89 	0.83 	0.86 
MISC 	0.99 	0.86 	0.79 	0.83 
O    	0.98 	0.98 	0.99 	0.99 
micro	0.99 	0.97 	0.97 	0.97 
macro	0.99 	0.90 	0.85 	0.88 
not-O	0.99 	0.89 	0.83 	0.86 

Entity level P/R/F1: 0.87/0.82/0.85

\end{verbatim}

    Viterbi decoding output:

    \begin{verbatim}
        Token-level confusion matrix:
go\gu   	PER     	ORG     	LOC     	MISC    	O       
PER     	2779.00 	54.00   	69.00   	22.00   	225.00  
ORG     	109.00  	1615.00 	78.00   	43.00   	247.00  
LOC     	41.00   	100.00  	1730.00 	27.00   	196.00  
MISC    	39.00   	36.00   	25.00   	1009.00 	159.00  
O       	53.00   	90.00   	42.00   	63.00   	42511.00

Token-level scores:
label	acc  	prec 	rec  	f1   
PER  	0.99 	0.92 	0.88 	0.90 
ORG  	0.99 	0.85 	0.77 	0.81 
LOC  	0.99 	0.89 	0.83 	0.86 
MISC 	0.99 	0.87 	0.80 	0.83 
O    	0.98 	0.98 	0.99 	0.99 
micro	0.99 	0.97 	0.97 	0.97 
macro	0.99 	0.90 	0.85 	0.88 
not-O	0.99 	0.89 	0.83 	0.86 

Entity level P/R/F1: 0.87/0.83/0.85

    \end{verbatim}

    \item[(e)] 
\end{itemize}

\section*{5. BiLSTM Tagger}

\begin{itemize}
    \item[(a)]
    \item[(b)]
    \item[(c)]
\end{itemize}

\end{document}